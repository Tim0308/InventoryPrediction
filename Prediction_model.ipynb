{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1412,
     "status": "ok",
     "timestamp": 1753854816892,
     "user": {
      "displayName": "Tim",
      "userId": "00272824328538595921"
     },
     "user_tz": -480
    },
    "id": "3XVlPiMj-Reg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\x281138\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lightgbm) (2.3.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\x281138\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lightgbm) (1.16.0)\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.5/1.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 3.9 MB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uT1Ica8g5BH-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   ---------------------------------------- 2/2 [openpyxl]\n",
      "\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7z0DRUGTrll"
   },
   "outputs": [],
   "source": [
    "# Replace 'path/to/your/excel_file.xlsx' with the actual path to your file in Google Drive\n",
    "Innovative_BU_erbitux_sales_2023_to_2025_HA_path = '/content/drive/MyDrive/Merck/Innovative_BU_erbitux_sales_2023_to_2025_HA.xlsx'\n",
    "# Replace 'Sheet1' with the name or index of the sheet you want to read\n",
    "Innovative_BU_erbitux_sales_2023_to_2025_HA = pd.read_excel(Innovative_BU_erbitux_sales_2023_to_2025_HA_path)\n",
    "\n",
    "display(Innovative_BU_erbitux_sales_2023_to_2025_HA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R5EobgT2TrjR"
   },
   "outputs": [],
   "source": [
    "# ['PRINCESS MARGARET HOSP', 'UNITED CHRISTIAN HOSPITAL', 'TSEUNG KWAN O HOSPITAL', 'QUEEN ELIZABETH HOSPITAL', 'PRINCE OF WALES HOSPITAL', 'QUEEN MARY HOSPITAL C/O PHARMACY', 'PAMELA YOUDE NETHERSOLE EASTERN HOSPITAL', 'TUEN MUN HOSPITAL']\n",
    "HA_hospital_list = ['UNITED CHRISTIAN HOSPITAL', 'TSEUNG KWAN O HOSPITAL', 'QUEEN ELIZABETH HOSPITAL']\n",
    "\n",
    "# top_sales_hospital = ['QUEEN ELIZABETH HOSPITAL']\n",
    "HA_hospital_table_list = []\n",
    "\n",
    "\n",
    "# # Filter the DataFrame based on 'Brand Detail' being in the list and 'AmountInHKD' being greater than 0\n",
    "\n",
    "for HA_hospital in HA_hospital_list:\n",
    "  table_name = f'Innovative_BU_erbitux_sales_2023_to_2025_HA_{HA_hospital}'\n",
    "  table_name = Innovative_BU_erbitux_sales_2023_to_2025_HA[\n",
    "      (Innovative_BU_erbitux_sales_2023_to_2025_HA['SoldToCustomerName'] == HA_hospital) &\n",
    "      # (Innovative_BU_erbitux_sales_2023_to_2025_HA['ShipToCode From ImportData'] == '70145698') &\n",
    "      (Innovative_BU_erbitux_sales_2023_to_2025_HA['Brand Detail'] == 'ERBITUX 5MG/ML INJ 20ML 1\\'S')\n",
    "  ]\n",
    "  # Innovative_BU_erbitux_sales_2023_to_2025_HA_filtered = Innovative_BU_erbitux_sales_2023_to_2025_HA_filtered.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "  # # Display the filtered DataFrame\n",
    "  # display(table_name.head())\n",
    "\n",
    "  print(f\"The number of rows in the Innovative_BU_erbitux_sales_2023_to_2025_HA_{HA_hospital} is: {table_name.shape[0]}\")\n",
    "  HA_hospital_table_list.append(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCrG60vUTrgc"
   },
   "outputs": [],
   "source": [
    "# write a for loop to iterate via each of the table in the HA_hospital_table_list to filter out colum \"SoldToCustomerName\", \"Brand Detail\", \"Std Counting Unit\", \"InvoiceDate\", \"SaleOrderNo\", \"Counting Unit\", \"AmountInHKD\", \"YearQuarter\" and \"Time_Between_Invoices\"\n",
    "\n",
    "# Columns to filter out\n",
    "columns_to_filter = ['Year', 'Quarter', 'Month',  'Original BU', 'ShipToCustomerName',\n",
    "                   'SoldToCode From ImportData', 'ShipToCode From ImportData', 'QTY',\n",
    "                   'Std/ Bonus...',  'Sales Rep', 'Sub Channel', 'LocationDesc']\n",
    "\n",
    "# Create a new list to store the dataframes with filtered columns\n",
    "HA_hospital_table_list_filtered = []\n",
    "\n",
    "# Iterate through each table in the HA_hospital_table_list\n",
    "for table in HA_hospital_table_list:\n",
    "  # Drop the specified columns from the current table\n",
    "  table_filtered = table.drop(columns=columns_to_filter, errors='ignore')  # Use errors='ignore' to avoid errors if a column doesn't exist\n",
    "  # Append the filtered table to the new list\n",
    "  HA_hospital_table_list_filtered.append(table_filtered)\n",
    "\n",
    "# Display the first few rows of the first filtered table as an example\n",
    "display(HA_hospital_table_list_filtered[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RaUgTFwYT_eJ"
   },
   "outputs": [],
   "source": [
    "# prompt: from HA_hospital_table_list[2], the columns SaleOrderNo consist of duplicates, merge the row with the same SaleOrderNo together. their Counting Unit, AmountInHKD, Std Counting Unit and  Time_Between_Invoices should be sum up tgt. the ended table should be short by InvoiceDate\n",
    "\n",
    "HA_hospital_table_list_merged_same_order = []\n",
    "\n",
    "for df_hospital in HA_hospital_table_list_filtered:\n",
    "  # Group by 'SaleOrderNo' and sum the specified columns\n",
    "  df_merged = df_hospital.groupby(['SaleOrderNo', 'InvoiceDate', 'SoldToCustomerName', 'Brand Detail']).agg(\n",
    "      {'Counting Unit': 'sum', 'AmountInHKD': 'sum', 'Std Counting Unit': 'sum', 'Time_Between_Invoices': 'sum'}\n",
    "  ).reset_index()\n",
    "\n",
    "  # Sort by 'InvoiceDate'\n",
    "  df_merged = df_merged.sort_values(by='InvoiceDate')\n",
    "\n",
    "  HA_hospital_table_list_merged_same_order.append(df_merged)\n",
    "\n",
    "\n",
    "display(HA_hospital_table_list_merged_same_order[0])\n",
    "print(f\"The number of rows in the merged and sorted DataFrame for {HA_hospital_list[0]} is: {HA_hospital_table_list_merged_same_order[0].shape[0]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PblffGlQT_c2"
   },
   "outputs": [],
   "source": [
    "# Visualize the time interval between each invoice date and AmountInHKD\n",
    "# skip the first row as its time_between_invoices is always 0\n",
    "\n",
    "fig = px.scatter(HA_hospital_table_list_merged_same_order[2].iloc[1:].dropna(),\n",
    "                 x='InvoiceDate',\n",
    "                 y='Time_Between_Invoices',\n",
    "                #  color='AmountInHKD',\n",
    "                 size='AmountInHKD',\n",
    "                 title='Time Between Invoices vs. Invoice Date colored by Amount in HKD',\n",
    "                 labels={'InvoiceDate': 'Invoice Date', 'Time_Between_Invoices': 'Time Between Invoices (Days)', 'AmountInHKD': 'Amount in HKD'})\n",
    "\n",
    "fig.update_traces(mode='lines+markers') # Add lines connecting the markers\n",
    "\n",
    "# Update layout for x-axis to show ticks every 1 month\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        # Removed tickmode='auto'\n",
    "        dtick='M2',  # Set x-axis ticks every 1 month\n",
    "        title='Invoice Date'\n",
    "    ),\n",
    "    yaxis_title=\"Time Between Invoices (Days)\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhSvWXQhUK0-"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from scipy.stats import randint, uniform\n",
    "import numpy as np\n",
    "\n",
    "def enhanced_RF(sales_table):\n",
    "    # Assuming sales_table is already created and has 'InvoiceDate', 'Time_Between_Invoices', and 'Counting Unit'\n",
    "    # Drop rows with NaN in 'Time_Between_Invoices' as the first row will have NaN\n",
    "    hospital_name = sales_table['SoldToCustomerName'].iloc[0]\n",
    "    print(f\"Processing: {hospital_name}\")\n",
    "    df_model_advanced = sales_table.dropna(subset=['Time_Between_Invoices']).copy()\n",
    "\n",
    "    # Convert InvoiceDate to a numerical representation (days since the earliest date in the dataset)\n",
    "    # We still calculate this for potential future use or visualization, but won't predict it directly\n",
    "    if 'earliest_date' not in locals():\n",
    "        earliest_date = df_model_advanced['InvoiceDate'].min()\n",
    "    df_model_advanced['Days_Since_Earliest'] = (df_model_advanced['InvoiceDate'] - earliest_date).dt.days\n",
    "\n",
    "\n",
    "    # Advanced Feature Engineering\n",
    "\n",
    "    # Lagged features (previous 1, 2, and 3 invoices)\n",
    "    for i in range(1, 4):\n",
    "        df_model_advanced[f'Lag_{i}_Time_Between_Invoices'] = df_model_advanced['Time_Between_Invoices'].shift(i)\n",
    "        df_model_advanced[f'Lag_{i}_Counting_Unit'] = df_model_advanced['Counting Unit'].shift(i)\n",
    "\n",
    "    # Rolling mean features (window of 3 invoices)\n",
    "    window_size = 3\n",
    "    df_model_advanced['Rolling_Mean_Time_Between_Invoices'] = df_model_advanced['Time_Between_Invoices'].rolling(window=window_size).mean()\n",
    "    df_model_advanced['Rolling_Mean_Counting_Unit'] = df_model_advanced['Counting Unit'].rolling(window=window_size).mean()\n",
    "\n",
    "\n",
    "    # Drop rows with NaN created by shifting and rolling\n",
    "    df_model_advanced.dropna(inplace=True)\n",
    "\n",
    "    # Select features (X) and target (y)\n",
    "    # The target is now 'Time_Between_Invoices'\n",
    "    X_advanced = df_model_advanced[['Lag_1_Time_Between_Invoices', 'Lag_2_Time_Between_Invoices', 'Lag_3_Time_Between_Invoices',\n",
    "                                   'Lag_1_Counting_Unit', 'Lag_2_Counting_Unit', 'Lag_3_Counting_Unit',\n",
    "                                   'Rolling_Mean_Time_Between_Invoices', 'Rolling_Mean_Counting_Unit',\n",
    "                                   'Counting Unit']] # Include the current Counting Unit as a feature\n",
    "    y_advanced = df_model_advanced['Time_Between_Invoices'] # Target is Time_Between_Invoices\n",
    "\n",
    "\n",
    "    # Time-based split\n",
    "    # We still need a train/test split for final evaluation after tuning\n",
    "    split_index_advanced = int(len(df_model_advanced) * 0.83)\n",
    "    X_train_advanced, X_test_advanced = X_advanced[:split_index_advanced], X_advanced[split_index_advanced:]\n",
    "    y_train_advanced, y_test_advanced = y_advanced[:split_index_advanced], y_advanced[split_index_advanced:]\n",
    "\n",
    "    print(\"Training set shapes for predicting Time_Between_Invoices:\")\n",
    "    print(\"X_train_advanced:\", X_train_advanced.shape)\n",
    "    print(\"y_train_advanced:\", y_train_advanced.shape)\n",
    "    print(\"\\nTesting set shapes for predicting Time_Between_Invoices:\")\n",
    "    print(\"X_test_advanced:\", X_test_advanced.shape)\n",
    "    y_test_advanced = y_test_advanced.reset_index(drop=True) # Reset index to align with new df\n",
    "    print(\"y_test_advanced:\", y_test_advanced.shape)\n",
    "\n",
    "\n",
    "    # Define the parameter distribution for Randomized Search for Random Forest\n",
    "    param_dist_rf = {\n",
    "        'n_estimators': randint(100, 1000),\n",
    "        'max_features': ['sqrt', 'log2'], # Features to consider at each split (removed 'auto' as it's deprecated)\n",
    "        'max_depth': randint(10, 110),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'min_samples_leaf': randint(1, 20),\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    # Initialize the Random Forest Regressor\n",
    "    rf_tuned = RandomForestRegressor(random_state=42)\n",
    "\n",
    "    # Initialize TimeSeriesSplit for cross-validation\n",
    "    # n_splits determines the number of splits. Increase for more robust evaluation.\n",
    "    tscv = TimeSeriesSplit(n_splits=5) # Using TimeSeriesSplit\n",
    "\n",
    "    # Initialize RandomizedSearchCV\n",
    "    random_search_rf = RandomizedSearchCV(\n",
    "        rf_tuned,\n",
    "        param_distributions=param_dist_rf,\n",
    "        n_iter=100,  # Number of different combinations to try (can adjust)\n",
    "        cv=tscv,     # Use TimeSeriesSplit for cross-validation\n",
    "        scoring='neg_mean_squared_error', # Use negative MSE for scoring\n",
    "        random_state=42,\n",
    "        n_jobs=-1    # Use all available cores\n",
    "    )\n",
    "\n",
    "    # Perform randomized search on the training data\n",
    "    # We fit on the training data because TimeSeriesSplit will handle the temporal splits within the training set\n",
    "    random_search_rf.fit(X_train_advanced, y_train_advanced)\n",
    "\n",
    "    # Get the best parameters and the best model\n",
    "    best_params_rf = random_search_rf.best_params_\n",
    "    # best_params_rf = {'bootstrap': False, 'max_depth': 105, 'max_features': 'log2', 'min_samples_leaf': 5, 'min_samples_split': 8, 'n_estimators': 848}\n",
    "    best_rf_model_tuned = random_search_rf.best_estimator_\n",
    "    # best_rf_model_tuned = RandomForestRegressor(bootstrap=False, max_depth=105, max_features='log2', min_samples_leaf=5, min_samples_split=8, n_estimators=848, random_state=42)\n",
    "\n",
    "    print(\"\\nBest hyperparameters found for tuned Random Forest:\", best_params_rf)\n",
    "    print(\"\\nBest hyperparameters found for best_rf_model_tuned:\", best_rf_model_tuned)\n",
    "    # Best hyperparameters found for tuned Random Forest: {'bootstrap': False, 'max_depth': 105, 'max_features': 'log2', 'min_samples_leaf': 5, 'min_samples_split': 8, 'n_estimators': 848}\n",
    "    # RandomForestRegressor(bootstrap=False, max_depth=105, max_features='log2', min_samples_leaf=5, min_samples_split=8, n_estimators=848, random_state=42)\n",
    "\n",
    "\n",
    "    # Make predictions on the test set using the best tuned model\n",
    "    y_pred_time_interval_rf_tuned = best_rf_model_tuned.predict(X_test_advanced)\n",
    "\n",
    "    # Evaluate the best tuned Random Forest model (predicting Time_Between_Invoices)\n",
    "    mse_time_interval_rf_tuned = mean_squared_error(y_test_advanced, y_pred_time_interval_rf_tuned)\n",
    "    rmse_time_interval_rf_tuned = np.sqrt(mse_time_interval_rf_tuned)\n",
    "    r2_time_interval_rf_tuned = r2_score(y_test_advanced, y_pred_time_interval_rf_tuned)\n",
    "\n",
    "    print(f\"\\nTuned Random Forest Model predicting Time_Between_Invoices:\")\n",
    "    print(f\"Mean Squared Error: {mse_time_interval_rf_tuned:.2f}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse_time_interval_rf_tuned:.2f}\")\n",
    "    print(f\"R-squared: {r2_time_interval_rf_tuned:.2f}\")\n",
    "\n",
    "    # Display predictions vs actual time intervals for inspection\n",
    "    results_df_time_interval_rf_tuned = pd.DataFrame({'Actual_Time_Between': y_test_advanced, 'Predicted_Time_Between': y_pred_time_interval_rf_tuned})\n",
    "    # Add the corresponding InvoiceDate for visualization\n",
    "    results_df_time_interval_rf_tuned['InvoiceDate'] = df_model_advanced.iloc[split_index_advanced:]['InvoiceDate'].values\n",
    "    display(results_df_time_interval_rf_tuned.head())\n",
    "\n",
    "    # Visualize the actual vs predicted time intervals from tuned Random Forest\n",
    "    # fig_rf_tuned_interval = px.line(results_df_time_interval_rf_tuned, x='InvoiceDate', y=['Actual_Time_Between', 'Predicted_Time_Between'],\n",
    "    #               title='Actual vs Predicted Time Between Invoices (Tuned Random Forest)',\n",
    "    #               labels={'InvoiceDate': 'Invoice Date', 'value': 'Time Between Invoices (Days)'})\n",
    "\n",
    "    # fig_rf_tuned_interval.update_layout(\n",
    "    #     xaxis=dict(\n",
    "    #         dtick='M2',  # Set x-axis ticks every 2 months\n",
    "    #         title='Invoice Date'\n",
    "    #     ),\n",
    "    #     yaxis_title=\"Time Between Invoices (Days)\"\n",
    "    # )\n",
    "    # fig_rf_tuned_interval.show()\n",
    "\n",
    "\n",
    "    # To forecast the next invoice date:\n",
    "    # We need to predict the time interval for the next invoice.\n",
    "    # The features for the next prediction will be based on the last row of the df_model_advanced DataFrame.\n",
    "\n",
    "    last_row_advanced = df_model_advanced.iloc[-1]\n",
    "\n",
    "    # Prepare features for the next prediction (predicting the time interval)\n",
    "    # Features are: 'Lag_1_Time_Between_Invoices', 'Lag_2_Time_Between_Invoices', 'Lag_3_Time_Between_Invoices',\n",
    "    # 'Lag_1_Counting_Unit', 'Lag_2_Counting_Unit', 'Lag_3_Counting_Unit',\n",
    "    # 'Rolling_Mean_Time_Between_Invoices', 'Rolling_Mean_Counting_Unit', 'Counting Unit'\n",
    "\n",
    "    # For the next prediction:\n",
    "    # Lag_1_Time_Between_Invoices will be the last actual Time_Between_Invoices\n",
    "    # Lag_2_Time_Between_Invoices will be the last actual Lag_1_Time_Between_Invoices\n",
    "    # Lag_3_Time_Between_Invoices will be the last actual Lag_2_Time_Between_Invoices\n",
    "    # Similarly for Counting_Unit lagged features\n",
    "\n",
    "    # Rolling means will be recalculated including the last actual Time_Between_Invoices and Counting Unit\n",
    "    # The next 'Counting Unit' (the last feature) needs to be estimated. We'll use the average of the last few actual counting units.\n",
    "\n",
    "    last_actual_time_between = last_row_advanced['Time_Between_Invoices']\n",
    "    last_lag1_time_between = last_row_advanced['Lag_1_Time_Between_Invoices']\n",
    "    last_lag2_time_between = last_row_advanced['Lag_2_Time_Between_Invoices'] # This will be the Lag_3 for the next step\n",
    "\n",
    "    last_actual_counting_unit = last_row_advanced['Counting Unit']\n",
    "    last_lag1_counting_unit = last_row_advanced['Lag_1_Counting_Unit']\n",
    "    last_lag2_counting_unit = last_row_advanced['Lag_2_Counting_Unit'] # This will be the Lag_3 for the next step\n",
    "\n",
    "    # Recalculate rolling means including the last actual data\n",
    "    last_rolling_mean_time_between = df_model_advanced['Time_Between_Invoices'].tail(window_size).mean()\n",
    "    last_rolling_mean_counting_unit = df_model_advanced['Counting Unit'].tail(window_size).mean()\n",
    "\n",
    "    # Estimate the next Counting Unit (using the mean of the last window_size)\n",
    "    next_counting_unit_estimated = df_model_advanced['Counting Unit'].tail(window_size).mean()\n",
    "\n",
    "\n",
    "    next_features_time_interval = np.array([[\n",
    "        last_actual_time_between,        # Lag_1_Time_Between_Invoices for next step\n",
    "        last_lag1_time_between,          # Lag_2_Time_Between_Invoices for next step\n",
    "        last_lag2_time_between,          # Lag_3_Time_Between_Invoices for next step\n",
    "        last_actual_counting_unit,       # Lag_1_Counting_Unit for next step\n",
    "        last_lag1_counting_unit,         # Lag_2_Counting_Unit for next step\n",
    "        last_lag2_counting_unit,         # Lag_3_Counting_Unit for next step\n",
    "        last_rolling_mean_time_between,  # Rolling_Mean_Time_Between_Invoices for next step\n",
    "        last_rolling_mean_counting_unit, # Rolling_Mean_Counting_Unit for next step\n",
    "        next_counting_unit_estimated     # Estimated Counting Unit for the next invoice\n",
    "    ]])\n",
    "\n",
    "\n",
    "    predicted_next_time_interval_rf_tuned = best_rf_model_tuned.predict(next_features_time_interval)[0]\n",
    "\n",
    "    # Ensure the predicted time interval is not negative\n",
    "    predicted_next_time_interval_rf_tuned = max(0, predicted_next_time_interval_rf_tuned)\n",
    "\n",
    "\n",
    "    # Get the last actual invoice date\n",
    "    last_actual_invoice_date = df_model_advanced['InvoiceDate'].iloc[-1]\n",
    "\n",
    "    # Forecast the next invoice date by adding the predicted time interval to the last actual invoice date\n",
    "    forecasted_next_invoice_date_rf_tuned = last_actual_invoice_date + pd.to_timedelta(predicted_next_time_interval_rf_tuned, unit='D')\n",
    "\n",
    "    print(f\"\\nLast Actual Invoice Date: {last_actual_invoice_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Predicted Time Interval for Next Invoice: {predicted_next_time_interval_rf_tuned:.2f} days\")\n",
    "    print(f\"Forecasted Next Invoice Date (using Tuned Random Forest model): {forecasted_next_invoice_date_rf_tuned.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "    # Visualize the actual vs predicted time intervals and the forecasted date\n",
    "    results_melted_forecast = results_df_time_interval_rf_tuned.melt(id_vars='InvoiceDate', var_name='Type', value_name='Time_Between_Invoices')\n",
    "\n",
    "    # Add the forecast point to the melted dataframe\n",
    "    forecast_data = pd.DataFrame({\n",
    "        'InvoiceDate': [forecasted_next_invoice_date_rf_tuned],\n",
    "        'Time_Between_Invoices': [predicted_next_time_interval_rf_tuned],\n",
    "        'Type': ['Forecasted Next Invoice Date (RF)']\n",
    "    })\n",
    "\n",
    "    results_melted_forecast = pd.concat([results_melted_forecast, forecast_data], ignore_index=True)\n",
    "\n",
    "    # Add an 'Invoice_Order' column for the x-axis\n",
    "    # We need to re-calculate this based on the combined dataframe for correct sequential numbering\n",
    "    results_melted_forecast = results_melted_forecast.sort_values(by='InvoiceDate').reset_index(drop=True)\n",
    "    results_melted_forecast['Invoice_Order'] = results_melted_forecast.index + 1\n",
    "\n",
    "\n",
    "    fig_forecast = px.line(results_melted_forecast,\n",
    "                          x='Invoice_Order',\n",
    "                          y='Time_Between_Invoices',\n",
    "                          color='Type',\n",
    "                          title='Actual vs Predicted Time Between Invoices and Forecasted Next Date (Tuned RF)',\n",
    "                          labels={'Invoice_Order': 'Invoice Number', 'Time_Between_Invoices': 'Time Between Invoices (Days)'},\n",
    "                          custom_data=['InvoiceDate']) # Include InvoiceDate in custom data for hover\n",
    "\n",
    "    # Customize hover template to show Invoice Date\n",
    "    fig_forecast.update_traces(hovertemplate=\"Invoice Number: %{x}<br>Time Between Invoices: %{y:.2f} days<br>Invoice Date: %{customdata[0]|%Y-%m-%d}<extra></extra>\")\n",
    "\n",
    "\n",
    "    # Add markers for all points, especially the forecast point\n",
    "    fig_forecast.update_traces(mode='lines+markers', selector=dict(type='scatter', mode='lines+markers'))\n",
    "    fig_forecast.update_traces(mode='markers', selector=dict(type='scatter', name='Forecasted Next Invoice Date (RF)')) # Ensure forecast has a marker\n",
    "\n",
    "\n",
    "    fig_forecast.update_layout(xaxis_title=\"Invoice Number\", yaxis_title=\"Time Between Invoices (Days)\")\n",
    "\n",
    "\n",
    "    fig_forecast.show()\n",
    "    ############################################################################################################################################\n",
    "\n",
    "    fig_forecast_2 = px.line(results_melted_forecast, x='InvoiceDate', y='Time_Between_Invoices', color='Type',\n",
    "                          title=f\"Actual vs Predicted Time Between Invoices and Forecasted Next Date (Tuned RF) for {hospital_name}\",\n",
    "                          labels={'InvoiceDate': 'Invoice Date', 'Time_Between_Invoices': 'Time Between Invoices (Days)'})\n",
    "\n",
    "    # Add markers for all points, especially the forecast point\n",
    "    fig_forecast_2.update_traces(mode='lines+markers', selector=dict(type='scatter', mode='lines+markers'))\n",
    "    fig_forecast_2.update_traces(mode='markers', selector=dict(type='scatter', name='Forecasted Next Invoice Date (RF)')) # Ensure forecast has a marker\n",
    "\n",
    "    fig_forecast_2.update_layout(\n",
    "        xaxis=dict(\n",
    "            dtick='M1',  # Set x-axis ticks every 2 months\n",
    "            title='Invoice Date'\n",
    "        ),\n",
    "        yaxis_title=\"Time Between Invoices (Days)\"\n",
    "    )\n",
    "    fig_forecast_2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQfehNTNUM4m"
   },
   "outputs": [],
   "source": [
    "for hospital_table in HA_hospital_table_list_merged_same_order[:1]:\n",
    "  display(hospital_table.iloc[1:].head(3))\n",
    "  results_melted_forecast_2 = enhanced_RF(hospital_table.iloc[1:]) # drop the first row as it is always 0 in time intervals\n",
    "  print(\"***********************************************************************************************************************************************\")\n",
    "  print(\"***********************************************************************************************************************************************\")\n",
    "  print(\"====================================================================================================================================================================== \\n\")\n",
    "  print(\"====================================================================================================================================================================== \\n\")\n",
    "  print(\"====================================================================================================================================================================== \\n\")\n",
    "  print(\"====================================================================================================================================================================== \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vn9GaMIUWUh"
   },
   "outputs": [],
   "source": [
    "display(results_melted_forecast_2)\n",
    "\n",
    "fig_forecast_2 = px.line(results_melted_forecast_2, x='InvoiceDate', y='Time_Between_Invoices', color='Type',\n",
    "                      title=f\"Actual vs Predicted Time Between Invoices and Forecasted Next Date (Tuned RF)\",\n",
    "                      labels={'InvoiceDate': 'Invoice Date', 'Time_Between_Invoices': 'Time Between Invoices (Days)'})\n",
    "\n",
    "# Add markers for all points, especially the forecast point\n",
    "fig_forecast_2.update_traces(mode='lines+markers', selector=dict(type='scatter', mode='lines+markers'))\n",
    "fig_forecast_2.update_traces(mode='markers', selector=dict(type='scatter', name='Forecasted Next Invoice Date (RF)')) # Ensure forecast has a marker\n",
    "\n",
    "fig_forecast_2.update_layout(\n",
    "    xaxis=dict(\n",
    "        dtick='M1',  # Set x-axis ticks every 2 months\n",
    "        title='Invoice Date'\n",
    "    ),\n",
    "    yaxis_title=\"Time Between Invoices (Days)\"\n",
    ")\n",
    "fig_forecast_2.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPctHPU3ScUIMUEj3t8F4jH",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
